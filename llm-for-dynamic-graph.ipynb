{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6266221,"sourceType":"datasetVersion","datasetId":3601853},{"sourceId":8574641,"sourceType":"datasetVersion","datasetId":5127309}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers import PreTrainedTokenizer\nimport os\nimport logging\n\ndef load_jaccard_scores(file_path):\n    jaccard_scores = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            scores = list(map(float, line.strip().split()))\n            jaccard_scores.append(scores)\n    return jaccard_scores\n\n# Function to select K examples based on precomputed Jaccard scores\ndef select_k_examples_precomputed(k, jaccard_scores, target_index):\n    scores = jaccard_scores[target_index]\n    # Get indices of top K scores\n    selected_example_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n    return selected_example_indices\n\n\n# Function to create the prompt with K selected examples\ndef create_few_shot_prompt(k, examples, target_example, jaccard_scores, target_index):\n    selected_example_indices = select_k_examples_precomputed(k, jaccard_scores, target_index)\n    selected_examples = [examples[i] for i in selected_example_indices]\n    instruction_prompt = \"This dynamic graph that the interaction of node change overtime.\"\n    example_prompts = [ex for ex in selected_examples]\n    predict_prompt = \" Belows are some examples of the node have similar interactions with target node.\"\n    full_prompt = instruction_prompt + predict_prompt + \"\\n\".join(example_prompts) + \"\\n\" + f\"Predict top 5 node ids will interaction with this target nodes: {target_example}.\"\n    return full_prompt\n\n# Custom dataset to handle the prompts\nclass PromptDataset(Dataset):\n    def __init__(self, examples_file_path, target_example_file_path, k, jaccard_scores):\n        with open(examples_file_path, encoding=\"utf-8\") as f:\n            examples = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n        self.examples = examples\n\n        with open(target_example_file_path, encoding=\"utf-8\") as f:\n            target_examples = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n        self.target_examples = target_examples[:5]\n\n        self.k = k\n        self.jaccard_scores = jaccard_scores\n\n    def __len__(self):\n        return len(self.target_examples)\n\n    def __getitem__(self, idx):\n        target_example = self.target_examples[idx]\n        prompt = create_few_shot_prompt(self.k, self.examples, target_example, self.jaccard_scores, idx)\n#         print(prompt)\n        return prompt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport torch\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\n\n# Initialize model and tokenizer\nmodel_name = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"  # Ensure this is the correct model name\nmodel = LlamaForCausalLM.from_pretrained(model_name)\ntokenizer = LlamaTokenizer.from_pretrained(model_name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = 1\nbatch_size = 1\nexamples_file_path = '/kaggle/input/uci-13/UCI_13/12/train.link_prediction'\ntarget_example_file_path='/kaggle/input/uci-13/UCI_13/12/test.link_prediction'\njaccard_scores_file = '/kaggle/input/uci-13/UCI_13/12/test.similar_score_jarcard'\njaccard_scores = load_jaccard_scores(jaccard_scores_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(jaccard_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to predict the next interaction for a batch of prompts\ndef predict_next_interaction_batch(model, tokenizer, prompts, max_length=1024):\n    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=False, truncation=True, max_length=1000)\n    outputs = model.generate(**inputs, max_length=max_length)\n    predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return predictions\n\n# Create DataLoader for batching\nprompt_dataset = PromptDataset(examples_file_path, target_example_file_path, K, jaccard_scores)\nprompt_loader = DataLoader(prompt_dataset, batch_size=batch_size, shuffle=False)\n\n# Predict the next interaction for each batch\nall_predictions = []\nfor batch in prompt_loader:\n    predictions = predict_next_interaction_batch(model, tokenizer, batch)\n#     print(\"Prediction: \", predictions)\n    all_predictions.extend([predictions[0][len(batch[0]):]])\nfor i, prediction in enumerate(all_predictions):\n    print(f\"Target Example {i}: {prediction}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}